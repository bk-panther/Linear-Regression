{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating analytical_solution...\n",
      "analytical_solution \t train loss: 0.0000072296, dev_loss: 8.0581447619 \n",
      "training LR using gradient descent...\n",
      "step 0 \t dev loss: 7252.073406710706 \t train loss: 40297.424982539334\n",
      "step 10000 \t dev loss: 9.779444384830512 \t train loss: 5.076865221629813\n",
      "step 20000 \t dev loss: 8.454180350132562 \t train loss: 2.338349732630941\n",
      "step 30000 \t dev loss: 8.842297889736564 \t train loss: 1.4526437607665141\n",
      "step 40000 \t dev loss: 8.441756060905334 \t train loss: 1.1216760097796064\n",
      "step 50000 \t dev loss: 8.475664354466979 \t train loss: 0.9674275230444365\n",
      "evaluating iterative_solution...\n",
      "gradient_descent_soln \t train loss: 0.9674275230444365, dev_loss: 8.475664354466979 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Scaler():\n",
    "#     mean \n",
    "#     std \n",
    "    # hint: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "    def __call__(self,features, is_train=False):\n",
    "#         if(is_train):\n",
    "#             Scaler.mean = np.mean(features,axis=0)\n",
    "#             Scaler.std = np.std(features,axis=0)\n",
    "#         print(features.shape)\n",
    "#         features[:,0:] = (features[:,0:] - Scaler.mean)/ Scaler.std\n",
    "        min_f = np.min(features[:,0:])\n",
    "        max_f = np.max(features[:,0:])\n",
    "        features[:,0:] = (features[:,0:]-min_f)/(max_f-min_f)\n",
    "        return features\n",
    "\n",
    "\n",
    "def get_features(csv_path,is_train=False,scaler=None):\n",
    "    '''\n",
    "    Description:\n",
    "    read input feature columns from csv file\n",
    "    manipulate feature columns, create basis functions, do feature scaling etc.\n",
    "    return a feature matrix (numpy array) of shape m x n \n",
    "    m is number of examples, n is number of features\n",
    "    return value: numpy array\n",
    "    '''\n",
    "    '''\n",
    "    Arguments:\n",
    "    csv_path: path to csv file\n",
    "    is_train: True if using training data (optional)\n",
    "    scaler: a class object for doing feature scaling (optional)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    help:\n",
    "    useful links: \n",
    "        * https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "        * https://www.geeksforgeeks.org/python-read-csv-using-pandas-read_csv/\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path)\n",
    "    features = np.array(df.drop(' shares',axis = 1))\n",
    "    '''Scaling'''\n",
    "    scaler.__call__(features,is_train)\n",
    "    '''Bias column addtition'''\n",
    "    bias = np.ones((1,len(features[:,0])))\n",
    "    features = np.insert(features, 0, bias, axis=1)\n",
    "    return features\n",
    "    \n",
    "    \n",
    "\n",
    "def get_targets(csv_path):\n",
    "    '''\n",
    "    Description:\n",
    "    read target outputs from the csv file\n",
    "    return a numpy array of shape m x 1\n",
    "    m is number of examples\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path)\n",
    "    target = df[' shares'].values.reshape(-1,1)\n",
    "    Tnorm = target.copy()\n",
    "    max_t = np.max(target)\n",
    "    min_t = np.min(target)\n",
    "    Tnorm = (target-min_t)/(max_t - min_t)\n",
    "    return Tnorm\n",
    "     \n",
    "\n",
    "def analytical_solution(feature_matrix, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    implement analytical solution to obtain weights\n",
    "    as described in lecture 5d\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "    p1 = np.dot(feature_matrix.T,feature_matrix)\n",
    "    p2 = p1 + C*np.identity(len(feature_matrix[0]))\n",
    "    p3 = np.dot(feature_matrix.T,targets)\n",
    "    w_star = np.matmul(np.linalg.pinv(p2),p3)\n",
    "    return w_star\n",
    "\n",
    "def get_predictions(feature_matrix, weights):\n",
    "    '''\n",
    "    description\n",
    "    return predictions given feature matrix and weights\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "\n",
    "    pre = feature_matrix.dot(weights)\n",
    "    return pre\n",
    "\n",
    "def mse_loss(feature_matrix, weights, targets):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement mean squared error loss function\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "    loss = np.sum((feature_matrix.dot(weights)-targets)**2,axis=0)[0]\n",
    "    return loss  \n",
    "\n",
    "def l2_regularizer(weights):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement l2 regularizer\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "\n",
    "def loss_fn(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute the loss function: mse_loss + C * l2_regularizer\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "def compute_gradients(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute gradient of weights w.r.t. the loss_fn function implemented above\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: numpy array\n",
    "    '''\n",
    "    m = len(targets)\n",
    "#     grad = 1/m*np.sum((np.dot(feature_matrix,weights) - targets)*feature_matrix,axis=0).reshape(-1,1)\n",
    "\n",
    "    grad = (2/m)*np.dot(feature_matrix.T,(np.dot(feature_matrix,weights) - targets))\n",
    "    \n",
    "#     grad = grad + 2*C*weights\n",
    "    return grad\n",
    "    \n",
    "#     loss = np.subtract(np.dot(feature_matrix,weights),targets)\n",
    "#     gradient_w = np.dot(feature_matrix.T,loss) \n",
    "#     return gradient_w\n",
    "\n",
    "def sample_random_batch(feature_matrix, targets, batch_size):\n",
    "    '''\n",
    "    Description\n",
    "    Batching -- Randomly sample batch_size number of elements from feature_matrix and targets\n",
    "    return a tuple: (sampled_feature_matrix, sampled_targets)\n",
    "    sampled_feature_matrix: numpy array of shape batch_size x n\n",
    "    sampled_targets: numpy array of shape batch_size x 1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    batch_size: int\n",
    "    '''    \n",
    "    lb = np.random.randint(0, len(feature_matrix) - batch_size)\n",
    "    ub = lb + batch_size\n",
    "    return (feature_matrix[lb:ub], targets[lb:ub])\n",
    "    \n",
    "    \n",
    "def initialize_weights(n):\n",
    "    '''\n",
    "    Description:\n",
    "    initialize weights to some initial values\n",
    "    return value: numpy array of shape n x 1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    n: int\n",
    "    '''\n",
    "    weights = np.random.randn(n,1)\n",
    "    return weights\n",
    "    \n",
    "def update_weights(weights, gradients, lr):\n",
    "    '''\n",
    "    Description:\n",
    "    update weights using gradient descent\n",
    "    retuen value: numpy matrix of shape nx1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    # weights: numpy matrix of shape nx1\n",
    "    # gradients: numpy matrix of shape nx1\n",
    "    # lr: learning rate\n",
    "    '''    \n",
    "\n",
    "    weights = weights - lr*gradients\n",
    "    return weights\n",
    "\n",
    "def early_stopping(arg_1=None, arg_2=None, arg_3=None, arg_n=None):\n",
    "    # allowed to modify argument list as per your need\n",
    "    # return True or False\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def do_gradient_descent(train_feature_matrix,  \n",
    "                        train_targets, \n",
    "                        dev_feature_matrix,\n",
    "                        dev_targets,\n",
    "                        lr=1.0,\n",
    "                        C=0.0,\n",
    "                        batch_size=32,\n",
    "                        max_steps=10000,\n",
    "                        eval_steps=5):\n",
    "    '''\n",
    "    feel free to significantly modify the body of this function as per your needs.\n",
    "    ** However **, you ought to make use of compute_gradients and update_weights function defined above\n",
    "    return your best possible estimate of LR weights\n",
    "    a sample code is as follows -- \n",
    "    '''\n",
    "    weights = initialize_weights(len(train_feature_matrix[0]))\n",
    "    dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)\n",
    "    train_loss = mse_loss(train_feature_matrix, weights, train_targets)\n",
    "\n",
    "    print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(0,dev_loss,train_loss))\n",
    "    for step in range(1,max_steps+1):\n",
    "\n",
    "        #sample a batch of features and gradients\n",
    "        features,targets = sample_random_batch(train_feature_matrix,train_targets,batch_size)\n",
    "        \n",
    "        #compute gradients\n",
    "        gradients = compute_gradients(features, weights, targets, C)\n",
    "        \n",
    "        #update weights\n",
    "        weights = update_weights(weights, gradients, lr)\n",
    "\n",
    "        if step%eval_steps == 0:\n",
    "            dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)\n",
    "            train_loss = mse_loss(train_feature_matrix, weights, train_targets)\n",
    "            print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(step,dev_loss,train_loss))\n",
    "\n",
    "        '''\n",
    "        implement early stopping etc. to improve performance.\n",
    "        '''\n",
    "\n",
    "    return weights\n",
    "\n",
    "def do_evaluation(feature_matrix, targets, weights):\n",
    "    # your predictions will be evaluated based on mean squared error \n",
    "    predictions = get_predictions(feature_matrix, weights)\n",
    "    loss =  mse_loss(feature_matrix, weights, targets)\n",
    "    return loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scaler = Scaler() #use of scaler is optional\n",
    "    train_features, train_targets = get_features('train.csv',True,scaler), get_targets('train.csv')\n",
    "    dev_features, dev_targets = get_features('dev.csv',False,scaler), get_targets('dev.csv')\n",
    "\n",
    "    a_solution = analytical_solution(train_features, train_targets, C=1e-8)\n",
    "    print('evaluating analytical_solution...')\n",
    "    dev_loss=do_evaluation(dev_features, dev_targets, a_solution)\n",
    "    train_loss=do_evaluation(train_features, train_targets, a_solution)\n",
    "    print('analytical_solution \\t train loss: {:.10f}, dev_loss: {:.10f} '.format(train_loss, dev_loss))\n",
    "\n",
    "    print('training LR using gradient descent...')\n",
    "    gradient_descent_soln = do_gradient_descent(train_features, \n",
    "                        train_targets, \n",
    "                        dev_features,\n",
    "                        dev_targets,\n",
    "                        lr=0.1,\n",
    "                        C=1e-15,\n",
    "                        batch_size=32,\n",
    "                        max_steps=50000,\n",
    "                        eval_steps=10000)\n",
    "\n",
    "    print('evaluating iterative_solution...')\n",
    "    dev_loss=do_evaluation(dev_features, dev_targets, gradient_descent_soln)\n",
    "    train_loss=do_evaluation(train_features, train_targets, gradient_descent_soln)\n",
    "    print('gradient_descent_soln \\t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11894, 60)\n",
      "[[0.24159223]\n",
      " [0.17863273]]\n",
      "[[5504.66023413]\n",
      " [4463.12131597]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "fx = df.to_numpy()\n",
    "min_fx = np.min(fx[:,0:])\n",
    "max_fx = np.max(fx[:,0:])\n",
    "fx[:,0:] = (fx[:,0:]-min_fx)/(max_fx-min_fx)\n",
    "# Adding bias\n",
    "bias_x = np.ones((1,len(fx[:,0])))\n",
    "fx = np.insert(fx, 0, bias_x, axis=1)\n",
    "print(fx.shape)\n",
    "predictions = get_predictions(fx, a_solution)\n",
    "print(predictions[1:3])\n",
    "# Denormalize\n",
    "df = pd.read_csv('train.csv')\n",
    "target = df[' shares'].values.reshape(-1,1)\n",
    "max_t = np.max(target)\n",
    "min_t = np.min(target)\n",
    "predictions = (predictions*(max_t - min_t))+min_t\n",
    "print(predictions[1:3])\n",
    "csv = pd.DataFrame()\n",
    "csv['instance_id'] = [i for i in range(len(predictions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       instance_id       shares\n",
      "0                0  5995.413504\n",
      "1                1  5504.660234\n",
      "2                2  4463.121316\n",
      "3                3  5568.774562\n",
      "4                4  5962.962725\n",
      "...            ...          ...\n",
      "11889        11889  7017.171565\n",
      "11890        11890  5574.931788\n",
      "11891        11891  6187.125392\n",
      "11892        11892  1918.112203\n",
      "11893        11893  4729.084585\n",
      "\n",
      "[11894 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "csv['shares'] = predictions\n",
    "print(csv)\n",
    "csv.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import tkinter\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (23786, 1) \tfeatures shape: (23786, 59)\n",
      "targets shape: (23786, 1) \tfeatures shape: (23786, 60)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "features = train.values\n",
    "target = train[' shares'].values.reshape(-1,1)\n",
    "features = np.delete(features, 59, 1)\n",
    "# target = np.delete(target, np.s_[99:23786:1], 0)\n",
    "# features = np.delete(features, np.s_[99:23786:1], 0)\n",
    "print(\"targets shape:\",target.shape,\"\\tfeatures shape:\",features.shape)\n",
    "# print(\"vector=\",features[:,2])\n",
    "# print(\"mean=\",np.mean(features[:,2]),\"std=\",np.std(features[:,2]))\n",
    "\n",
    "# trial = (features[:,2] - np.mean(features[:,2])) / np.std(features[:,2])\n",
    "# mean = np.mean(features,axis=0)\n",
    "# std = np.std(features,axis=0)\n",
    "# features[:,0:] = (features[:,0:] - mean)/ std\n",
    "min_f = np.min(features[:,0:])\n",
    "max_f = np.max(features[:,0:])\n",
    "features[:,0:] = (features[:,0:]-min_f)/(max_f-min_f)\n",
    "# print(np.mean(features,axis=0))\n",
    "# print(np.std(features,axis=0))\n",
    "# max_t = np.max(features[:,2])\n",
    "# min_t = np.min(features[:,2])\n",
    "# print(\"mean=\",np.mean(features,axis=0),\"std=\",np.std(features[:,2]))\n",
    "# features[:,2] = (features[:,2]-min_t)/(max_t - min_t)\n",
    "\n",
    "# # print(\"vector=\",features[:,2])\n",
    "# print(\"mean=\",np.mean(features[:,2]),\"std=\",np.std(features[:,2]))\n",
    "# print(train.head())\n",
    "# list = []\n",
    "# for i in range(0,23786):\n",
    "#     list.append(i)\n",
    "# X = np.array(list).reshape(-1,1)\n",
    "# print(X.shape)\n",
    "# fig = plt.figure(1, figsize=(12,12))\n",
    "# ax = fig.add_subplot(121)\n",
    "# ax1 = fig.add_subplot(122)\n",
    "# ax.scatter(X,features[:,2],c='g')\n",
    "# ax1.scatter(X,features[:,2],c='r')\n",
    "# plt.show()\n",
    "bias = np.ones((1,len(features[:,0])))\n",
    "features = np.insert(features,0,bias,axis=1)\n",
    "# print(X[0:3,0:3])\n",
    "# print(np.std(X,axis=0))\n",
    "# print(target[1:4])\n",
    "# Tnorm = target.copy()\n",
    "max_t = np.max(target)\n",
    "min_t = np.min(target)\n",
    "target = (target-min_t)/(max_t - min_t)\n",
    "# max_t1 = np.max(Tnorm)\n",
    "# min_t1 = np.min(Tnorm)\n",
    "# print(min_t,max_t)\n",
    "# print(min_t1,max_t1)\n",
    "# weights = np.random.randn(len(features[0]),1)\n",
    "# print (0*weights)\n",
    "print(\"targets shape:\",target.shape,\"\\tfeatures shape:\",features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 1) \tX.W - Y == (23786, 1) \t(X.W-Y)*X.T == (60, 1)\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.randn(len(features[0,:]),1)\n",
    "m = len(target)\n",
    "p1 = features.dot(weights) - target\n",
    "p2 = np.dot(features.T,p1)\n",
    "# p3 = np.sum(p2,axis=0)\n",
    "# p1 = (feature_matrix.dot(weights) - targets)*feature_matrix\n",
    "# p2 = p1 + C*weights\n",
    "# grad = 1/m*np.sum((,axis=0) + C*weights).reshape(-1,1)\n",
    "# grad = 1/m*np.sum(p2,axis=0).reshape(-1,1)\n",
    "\n",
    "\n",
    "    \n",
    "# loss = np.subtract(np.dot(feature_matrix,weights),targets)\n",
    "# gradient_w = np.dot(feature_matrix.T,loss) \n",
    "# return gradient_w\n",
    "print(weights.shape,\"\\tX.W - Y ==\",p1.shape,\"\\t(X.W-Y)*X.T ==\",p2.shape)\n",
    "# print(\"sum() ==\",p3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xnorm= (23786, 60) ynorm= (23786, 1)\n",
      "theta= (60, 1)\n",
      "ypred= (23786, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((np.ones((len(features),1)),features),axis=1)\n",
    "Xnorm = X.copy()\n",
    "minx = np.min(X[:,1:])\n",
    "maxx = np.max(X[:,1:])\n",
    "Xnorm[:,1:] = (X[:,1:]-minx)/(maxx-minx)\n",
    "\n",
    "ynorm = target.copy()\n",
    "maxy = np.max(target)\n",
    "miny = np.min(target)\n",
    "ynorm = (target-miny)/(maxy - miny) \n",
    "print(\"xnorm=\",Xnorm.shape,\"ynorm=\",ynorm.shape)\n",
    "\n",
    "theta0 = np.zeros((X.shape[1],1))+0.4\n",
    "print(\"theta=\",theta0.shape)\n",
    "\n",
    "ypred = Xnorm.dot(theta0)\n",
    "print(\"ypred=\",ypred.shape)\n",
    "sortidx = np.argsort(ynorm[:,0]) # sort the values for better visualization\n",
    "plt.plot(ynorm[sortidx,0],'o')\n",
    "plt.plot(ypred[sortidx,0],'--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient\n",
    "def grad(theta):\n",
    "    dJ = 1/m*np.sum((Xnorm.dot(theta)-ynorm)*Xnorm,axis=0).reshape(-1,1)\n",
    "    return dJ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10360.122353200131"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost(theta):\n",
    "    J = np.sum((Xnorm.dot(theta)-ynorm)**2,axis=0)[0]\n",
    "    return J\n",
    "\n",
    "cost(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(theta0,learning_rate = 0.5,epochs=50000,TOL=1e-7):\n",
    "    \n",
    "    theta_history = [theta0]\n",
    "    cost_history = [cost(theta0)]\n",
    "    \n",
    "    theta_star = theta0*10000\n",
    "    print(f'epoch \\t Cost(J) \\t')\n",
    "    for epoch in range(epochs):\n",
    "        if epoch%100 == 0:\n",
    "            print(f'{epoch:5d}\\t{cost_history[-1]:7.4f}\\t')\n",
    "        dJ = grad(theta0)\n",
    "        J = cost(theta0)\n",
    "        \n",
    "        theta_star = theta0 - learning_rate*dJ\n",
    "        theta_history.append(theta_star)\n",
    "        cost_history.append(J)\n",
    "        \n",
    "        if np.sum((theta_star - theta0)**2) < TOL:\n",
    "            print('Convergence achieved.')\n",
    "            break\n",
    "        theta0 = theta_star\n",
    "\n",
    "    return theta_star,theta_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch \t Cost(J) \t\n",
      "    0\t10360.1224\t\n",
      "  100\t13.9842\t\n",
      "  200\t 8.5113\t\n",
      "  300\t 5.7978\t\n",
      "  400\t 4.1081\t\n",
      "  500\t 2.9595\t\n",
      "Convergence achieved.\n"
     ]
    }
   ],
   "source": [
    "theta,theta_history,cost_history = GD(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprednorm = Xnorm.dot(theta)\n",
    "\n",
    "ypred = yprednorm*(maxy-miny) + miny\n",
    "plt.plot(target[sortidx,0],'o')\n",
    "plt.plot(ypred[sortidx,0],'--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "# x = np.array(x)\n",
    "# y = np.array(y).reshape((100,1))\n",
    "# loss = np.subtract(np.dot(x,np.random.randn(len(x[0]),1)),y)\n",
    "df = pd.read_csv('train.csv')\n",
    "targets = df[' shares'].values.reshape(-1,1)\n",
    "\n",
    "feature_matrix = Xnorm.copy()\n",
    "\n",
    "# gradient_w = -2*np.dot(x.T,loss)\n",
    "# gradient_w * 2 * 0.1\n",
    "p1 = np.dot(feature_matrix.T,feature_matrix)\n",
    "p2 = p1 + 0.4*np.identity(len(feature_matrix[0]))\n",
    "p3 = np.dot(feature_matrix.T,targets)\n",
    "w_star = np.matmul(np.linalg.pinv(p2),p3)\n",
    "\n",
    "ypred=np.dot(feature_matrix,w_star)\n",
    "\n",
    "plt.plot(targets[sortidx,0],'o')\n",
    "plt.plot(ypred[sortidx,0],color='red')\n",
    "plt.title(\"Anal Solution\")\n",
    "plt.xlabel(\"Instances\")\n",
    "plt.ylabel(\"Targets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating analytical_solution...\n",
      "analytical_solution \t train loss: 0.0000072296, dev_loss: 8.0581447619 \n",
      "training LR using gradient descent...\n",
      "step 0 \t dev loss: 7252.0734072120495 \t train loss: 40297.42498304068\n",
      "step 10000 \t dev loss: 9.002619395421025 \t train loss: 2.4782901060593603\n",
      "step 20000 \t dev loss: 8.195423597281822 \t train loss: 1.1249270213045814\n",
      "step 30000 \t dev loss: 8.550062913324139 \t train loss: 0.8947122088158415\n",
      "step 40000 \t dev loss: 8.346548004688971 \t train loss: 0.7815384657053241\n",
      "step 50000 \t dev loss: 8.407301424665034 \t train loss: 0.7062740740048172\n",
      "step 60000 \t dev loss: 8.391298747365587 \t train loss: 0.6454784076485244\n",
      "step 70000 \t dev loss: 8.356206005561491 \t train loss: 0.5930995121059403\n",
      "step 80000 \t dev loss: 8.23379350048393 \t train loss: 0.5480855305361683\n",
      "step 90000 \t dev loss: 8.363872669964765 \t train loss: 0.5142450683451687\n",
      "step 100000 \t dev loss: 8.275305089689338 \t train loss: 0.48007380771007535\n",
      "step 110000 \t dev loss: 8.132025246003238 \t train loss: 0.4509302220244654\n",
      "step 120000 \t dev loss: 8.239309685930898 \t train loss: 0.4245435031867447\n",
      "step 130000 \t dev loss: 8.243561973396373 \t train loss: 0.3994975221192197\n",
      "step 140000 \t dev loss: 8.084931178650924 \t train loss: 0.3784708587444348\n",
      "step 150000 \t dev loss: 8.268645549764663 \t train loss: 0.35836529925230287\n",
      "step 160000 \t dev loss: 8.053487939192966 \t train loss: 0.340085969216188\n",
      "step 170000 \t dev loss: 7.751874598335237 \t train loss: 0.3521258646820848\n",
      "step 180000 \t dev loss: 8.221827345301378 \t train loss: 0.3031337916759816\n",
      "step 190000 \t dev loss: 8.177157797634663 \t train loss: 0.28768215517907764\n",
      "step 200000 \t dev loss: 8.187950567884215 \t train loss: 0.27363450568321435\n",
      "step 210000 \t dev loss: 8.153671991884849 \t train loss: 0.2598748935372188\n",
      "step 220000 \t dev loss: 8.077076977013705 \t train loss: 0.24775662124114034\n",
      "step 230000 \t dev loss: 8.133657145862555 \t train loss: 0.23598765895310703\n",
      "step 240000 \t dev loss: 8.113563578855992 \t train loss: 0.22499229482227043\n",
      "step 250000 \t dev loss: 8.090324569461961 \t train loss: 0.21449519203021825\n",
      "step 260000 \t dev loss: 8.155506429731807 \t train loss: 0.2040648944613782\n",
      "step 270000 \t dev loss: 8.058532972274834 \t train loss: 0.19506054416338797\n",
      "step 280000 \t dev loss: 7.979434599871997 \t train loss: 0.18895970215291583\n",
      "step 290000 \t dev loss: 8.167677217228313 \t train loss: 0.17756476842859592\n",
      "step 300000 \t dev loss: 8.164925825308721 \t train loss: 0.1697033484093183\n",
      "step 310000 \t dev loss: 8.205021022344923 \t train loss: 0.1631561491724135\n",
      "step 320000 \t dev loss: 8.095515792627051 \t train loss: 0.1542689025375023\n",
      "step 330000 \t dev loss: 8.08442020260105 \t train loss: 0.1472001773061812\n",
      "step 340000 \t dev loss: 8.063073323058621 \t train loss: 0.14098013780062185\n",
      "step 350000 \t dev loss: 8.146300055531661 \t train loss: 0.13462272282919577\n",
      "step 360000 \t dev loss: 8.061898402000876 \t train loss: 0.12824966835673876\n",
      "step 370000 \t dev loss: 8.031651419401866 \t train loss: 0.12347267083424576\n",
      "step 380000 \t dev loss: 8.154122034295602 \t train loss: 0.11776271657179498\n",
      "step 390000 \t dev loss: 8.144319304473793 \t train loss: 0.11227591166588645\n",
      "step 400000 \t dev loss: 8.067204947666983 \t train loss: 0.10726533555856084\n",
      "step 410000 \t dev loss: 8.073633706342871 \t train loss: 0.1026076442214543\n",
      "step 420000 \t dev loss: 8.15013025840685 \t train loss: 0.09887701108517037\n",
      "step 430000 \t dev loss: 8.127572280383331 \t train loss: 0.09401914748944387\n",
      "step 440000 \t dev loss: 7.879054306807307 \t train loss: 0.09826498097389695\n",
      "step 450000 \t dev loss: 8.141597504572692 \t train loss: 0.08636330654580991\n",
      "step 460000 \t dev loss: 8.01189423204477 \t train loss: 0.08347594922801957\n",
      "step 470000 \t dev loss: 8.066274509990881 \t train loss: 0.07895885026211275\n",
      "step 480000 \t dev loss: 8.001936084106847 \t train loss: 0.07704066691176467\n",
      "step 490000 \t dev loss: 8.051941829712147 \t train loss: 0.07256586617780729\n",
      "step 500000 \t dev loss: 8.10210867753777 \t train loss: 0.06965373864055197\n",
      "step 510000 \t dev loss: 8.131174177097286 \t train loss: 0.06705948376774766\n",
      "step 520000 \t dev loss: 8.098805431073389 \t train loss: 0.06383022500907556\n",
      "step 530000 \t dev loss: 8.113212394980996 \t train loss: 0.06141290066903936\n",
      "step 540000 \t dev loss: 8.085855892969521 \t train loss: 0.05866377325637137\n",
      "step 550000 \t dev loss: 8.039882375875068 \t train loss: 0.05676592186261402\n",
      "step 560000 \t dev loss: 8.099556086404775 \t train loss: 0.05427538402381714\n",
      "step 570000 \t dev loss: 8.074970413680296 \t train loss: 0.052052008417094904\n",
      "step 580000 \t dev loss: 8.145500614040678 \t train loss: 0.05078132377145413\n",
      "step 590000 \t dev loss: 7.9560544424615145 \t train loss: 0.05092665712033154\n",
      "step 600000 \t dev loss: 8.083922530804934 \t train loss: 0.04632782257711271\n",
      "step 610000 \t dev loss: 8.097848091141792 \t train loss: 0.04471358428349304\n",
      "step 620000 \t dev loss: 8.029221288336467 \t train loss: 0.04333242062018133\n",
      "step 630000 \t dev loss: 8.043265696575743 \t train loss: 0.04153633914595682\n",
      "step 640000 \t dev loss: 8.041644576072768 \t train loss: 0.04004073593386153\n",
      "step 650000 \t dev loss: 8.087358641960174 \t train loss: 0.038429338446756306\n",
      "step 660000 \t dev loss: 8.138244309749581 \t train loss: 0.037880284622896526\n",
      "step 670000 \t dev loss: 8.071374463554164 \t train loss: 0.03565432334568922\n",
      "step 680000 \t dev loss: 8.129589299621673 \t train loss: 0.03506768845394902\n",
      "step 690000 \t dev loss: 8.071170541049401 \t train loss: 0.03328024307464866\n",
      "step 700000 \t dev loss: 8.042633698586247 \t train loss: 0.03229781104710259\n",
      "step 710000 \t dev loss: 8.074669726650963 \t train loss: 0.03115985505181637\n",
      "step 720000 \t dev loss: 8.04602251430241 \t train loss: 0.030256047317373167\n",
      "step 730000 \t dev loss: 8.119543375000209 \t train loss: 0.029721216768654656\n",
      "step 740000 \t dev loss: 8.051994962029253 \t train loss: 0.02837526227910551\n",
      "step 750000 \t dev loss: 8.027369371392725 \t train loss: 0.027809611947580387\n",
      "step 760000 \t dev loss: 8.0850711372515 \t train loss: 0.026733522926754332\n",
      "step 770000 \t dev loss: 8.071356797579789 \t train loss: 0.02585418063241019\n",
      "step 780000 \t dev loss: 8.061876641783353 \t train loss: 0.025120216861949023\n",
      "step 790000 \t dev loss: 8.028455040115139 \t train loss: 0.024693967983984152\n",
      "step 800000 \t dev loss: 8.097352060456876 \t train loss: 0.023913274724350744\n",
      "step 810000 \t dev loss: 8.079456873005958 \t train loss: 0.02309690739282666\n",
      "step 820000 \t dev loss: 8.069296022273374 \t train loss: 0.022479923315750102\n",
      "step 830000 \t dev loss: 8.081868728846997 \t train loss: 0.02194836410452603\n",
      "step 840000 \t dev loss: 8.054947995652109 \t train loss: 0.021403750294233744\n",
      "step 850000 \t dev loss: 8.11700215868403 \t train loss: 0.02136475236446231\n",
      "step 860000 \t dev loss: 8.023760852581171 \t train loss: 0.020639514220073672\n",
      "step 870000 \t dev loss: 8.09225854349429 \t train loss: 0.01998224479716595\n",
      "step 880000 \t dev loss: 8.132395389889558 \t train loss: 0.020245643011921124\n",
      "step 890000 \t dev loss: 8.091536671173378 \t train loss: 0.019062880256680282\n",
      "step 900000 \t dev loss: 8.05225826375925 \t train loss: 0.01853995894653688\n",
      "step 910000 \t dev loss: 8.053157021963964 \t train loss: 0.018142125715764873\n",
      "step 920000 \t dev loss: 8.054918414068029 \t train loss: 0.017770690147912292\n",
      "step 930000 \t dev loss: 8.029977491175165 \t train loss: 0.01759941951154207\n",
      "step 940000 \t dev loss: 8.042385965630814 \t train loss: 0.017152943208107445\n",
      "step 950000 \t dev loss: 8.055574837544404 \t train loss: 0.016746008643925035\n",
      "step 960000 \t dev loss: 8.074722470815406 \t train loss: 0.01647038944410626\n",
      "step 970000 \t dev loss: 8.03423440609673 \t train loss: 0.01630246743211345\n",
      "step 980000 \t dev loss: 8.040194459817796 \t train loss: 0.01595786761458086\n",
      "step 990000 \t dev loss: 8.050996706672759 \t train loss: 0.015620106624635418\n",
      "step 1000000 \t dev loss: 8.012627058024037 \t train loss: 0.01580800055065498\n",
      "step 1010000 \t dev loss: 8.05692590426696 \t train loss: 0.015099520761184277\n",
      "step 1020000 \t dev loss: 8.075432877858528 \t train loss: 0.014897360250042926\n",
      "step 1030000 \t dev loss: 8.059741280241344 \t train loss: 0.014641031396241993\n",
      "step 1040000 \t dev loss: 8.081693502204676 \t train loss: 0.014518900617204688\n",
      "step 1050000 \t dev loss: 8.038959313672061 \t train loss: 0.014341939675097856\n",
      "step 1060000 \t dev loss: 8.057269459594457 \t train loss: 0.01406112887626104\n",
      "step 1070000 \t dev loss: 8.075384043849867 \t train loss: 0.01392261877210419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1080000 \t dev loss: 8.034983496412625 \t train loss: 0.013846124558257103\n",
      "step 1090000 \t dev loss: 8.071870697446114 \t train loss: 0.013567799123138023\n",
      "step 1100000 \t dev loss: 8.029618279125543 \t train loss: 0.013565882740424449\n",
      "step 1110000 \t dev loss: 8.086139378948149 \t train loss: 0.013354694074217674\n",
      "step 1120000 \t dev loss: 8.08479105378659 \t train loss: 0.013199637821221305\n",
      "step 1130000 \t dev loss: 8.068075450157936 \t train loss: 0.012961205425626123\n",
      "step 1140000 \t dev loss: 8.067403696538111 \t train loss: 0.012829080362403277\n",
      "step 1150000 \t dev loss: 8.057553841122937 \t train loss: 0.012703946429839559\n",
      "step 1160000 \t dev loss: 8.024108989477785 \t train loss: 0.012831436048099384\n",
      "step 1170000 \t dev loss: 8.06540950492891 \t train loss: 0.012465691326714382\n",
      "step 1180000 \t dev loss: 8.064042268149654 \t train loss: 0.012354069860635667\n",
      "step 1190000 \t dev loss: 8.07326396534038 \t train loss: 0.012279870947580965\n",
      "step 1200000 \t dev loss: 8.07513234419873 \t train loss: 0.012182762542241071\n",
      "step 1210000 \t dev loss: 8.076775276814592 \t train loss: 0.012100593166491156\n",
      "step 1220000 \t dev loss: 8.035720085156665 \t train loss: 0.01205779334045573\n",
      "step 1230000 \t dev loss: 8.044400608150974 \t train loss: 0.011890257288506925\n",
      "step 1240000 \t dev loss: 8.057052815986266 \t train loss: 0.011763871393325983\n",
      "step 1250000 \t dev loss: 8.052223549322168 \t train loss: 0.011691303498773087\n",
      "step 1260000 \t dev loss: 8.047850440059094 \t train loss: 0.01162291165436957\n",
      "step 1270000 \t dev loss: 8.072885765390108 \t train loss: 0.011559262867062397\n",
      "step 1280000 \t dev loss: 8.005129499417318 \t train loss: 0.012015289965494502\n",
      "step 1290000 \t dev loss: 8.05968366082143 \t train loss: 0.011386914913243052\n",
      "step 1300000 \t dev loss: 8.067202057777429 \t train loss: 0.01133507672498052\n",
      "step 1310000 \t dev loss: 8.038429226123517 \t train loss: 0.0113276919752307\n",
      "step 1320000 \t dev loss: 8.072699656794764 \t train loss: 0.011229286580913845\n",
      "step 1330000 \t dev loss: 8.049083860576873 \t train loss: 0.011157474766449061\n",
      "step 1340000 \t dev loss: 8.075194518721297 \t train loss: 0.01113641659130797\n",
      "step 1350000 \t dev loss: 8.032038993948674 \t train loss: 0.011160700254697094\n",
      "step 1360000 \t dev loss: 8.055652547516496 \t train loss: 0.010973736351771951\n",
      "step 1370000 \t dev loss: 8.068979377295344 \t train loss: 0.010944769638059418\n",
      "step 1380000 \t dev loss: 8.059581793655184 \t train loss: 0.01087723440385938\n",
      "step 1390000 \t dev loss: 8.047221425459002 \t train loss: 0.010856119355207961\n",
      "step 1400000 \t dev loss: 8.047172410142243 \t train loss: 0.010814022372746921\n",
      "step 1410000 \t dev loss: 8.070279026739074 \t train loss: 0.01077389803795798\n",
      "step 1420000 \t dev loss: 8.074739680549284 \t train loss: 0.010760328442859807\n",
      "step 1430000 \t dev loss: 8.029404134165151 \t train loss: 0.010823208994145687\n",
      "step 1440000 \t dev loss: 8.044489463549782 \t train loss: 0.01066141851722315\n",
      "step 1450000 \t dev loss: 8.049596728516073 \t train loss: 0.01060118029178148\n",
      "step 1460000 \t dev loss: 8.055408388851824 \t train loss: 0.010552615916420007\n",
      "step 1470000 \t dev loss: 8.064631231506342 \t train loss: 0.010528740801411616\n",
      "step 1480000 \t dev loss: 8.067640143359437 \t train loss: 0.010501267427423463\n",
      "step 1490000 \t dev loss: 8.048731724995013 \t train loss: 0.01046315746304258\n",
      "step 1500000 \t dev loss: 8.082446323495226 \t train loss: 0.010538881303586767\n",
      "step 1510000 \t dev loss: 8.055167547600606 \t train loss: 0.010386795506509464\n",
      "step 1520000 \t dev loss: 8.082228908274228 \t train loss: 0.010476139812960744\n",
      "step 1530000 \t dev loss: 8.062512157065079 \t train loss: 0.010331345278282607\n",
      "step 1540000 \t dev loss: 8.069245907057187 \t train loss: 0.010325820390407464\n",
      "step 1550000 \t dev loss: 8.074802944044546 \t train loss: 0.010330489296535286\n",
      "step 1560000 \t dev loss: 8.04253578543993 \t train loss: 0.010293783050566172\n",
      "step 1570000 \t dev loss: 8.044740522188892 \t train loss: 0.010258699080190824\n",
      "step 1580000 \t dev loss: 8.095485990512499 \t train loss: 0.010468785244660796\n",
      "step 1590000 \t dev loss: 8.064584231422133 \t train loss: 0.010184597931682379\n",
      "step 1600000 \t dev loss: 8.071650095259292 \t train loss: 0.010185644462436081\n",
      "step 1610000 \t dev loss: 8.051577586174098 \t train loss: 0.010132567890150596\n",
      "step 1620000 \t dev loss: 8.060963319254478 \t train loss: 0.010105116833621771\n",
      "step 1630000 \t dev loss: 8.064156286218932 \t train loss: 0.01009156190757913\n",
      "step 1640000 \t dev loss: 8.058840309824083 \t train loss: 0.01006167916450097\n",
      "step 1650000 \t dev loss: 8.046538303854998 \t train loss: 0.010065353492397688\n",
      "step 1660000 \t dev loss: 8.057425889386998 \t train loss: 0.01003207159231261\n",
      "step 1670000 \t dev loss: 8.095468289566089 \t train loss: 0.010280781739109976\n",
      "step 1680000 \t dev loss: 8.024661294365464 \t train loss: 0.010182693028587368\n",
      "step 1690000 \t dev loss: 8.05943762396961 \t train loss: 0.00996177496784584\n",
      "step 1700000 \t dev loss: 8.060025153226249 \t train loss: 0.009943721579419666\n",
      "step 1710000 \t dev loss: 8.053020650425045 \t train loss: 0.00992915122462319\n",
      "step 1720000 \t dev loss: 8.049346722988703 \t train loss: 0.009919089116746663\n",
      "step 1730000 \t dev loss: 8.094906335996809 \t train loss: 0.01015443437750054\n",
      "step 1740000 \t dev loss: 8.078217546094727 \t train loss: 0.009954547570539544\n",
      "step 1750000 \t dev loss: 8.092634516161867 \t train loss: 0.010097142729769738\n",
      "step 1760000 \t dev loss: 8.03796905101199 \t train loss: 0.009904832066398967\n",
      "step 1770000 \t dev loss: 8.034690861813491 \t train loss: 0.009922531995052216\n",
      "step 1780000 \t dev loss: 8.045689900545444 \t train loss: 0.009830247716867026\n",
      "step 1790000 \t dev loss: 8.075616087207232 \t train loss: 0.009859072520807019\n",
      "step 1800000 \t dev loss: 8.095570428808687 \t train loss: 0.010051090922631309\n",
      "step 1810000 \t dev loss: 8.083488270709166 \t train loss: 0.009894654182956434\n",
      "step 1820000 \t dev loss: 8.051732279464897 \t train loss: 0.009748339723534631\n",
      "step 1830000 \t dev loss: 8.065990487804067 \t train loss: 0.009744667116133393\n",
      "step 1840000 \t dev loss: 8.04133383751643 \t train loss: 0.009761364735955073\n",
      "step 1850000 \t dev loss: 8.020758474180056 \t train loss: 0.009945605526777214\n",
      "step 1860000 \t dev loss: 8.034222384501211 \t train loss: 0.009788111845948577\n",
      "step 1870000 \t dev loss: 8.031550100639612 \t train loss: 0.009792678718887445\n",
      "step 1880000 \t dev loss: 8.05553415954104 \t train loss: 0.009657871032741482\n",
      "step 1890000 \t dev loss: 8.039873320848942 \t train loss: 0.00969685649957927\n",
      "step 1900000 \t dev loss: 8.064125665723754 \t train loss: 0.009638028050754635\n",
      "step 1910000 \t dev loss: 8.075180737882533 \t train loss: 0.009679837847296622\n",
      "step 1920000 \t dev loss: 8.099096858096889 \t train loss: 0.009943434777894493\n",
      "step 1930000 \t dev loss: 8.05737628095717 \t train loss: 0.009586970733439979\n",
      "step 1940000 \t dev loss: 8.071517381578607 \t train loss: 0.009615946653965318\n",
      "step 1950000 \t dev loss: 8.032354710062389 \t train loss: 0.009680279378176289\n",
      "step 1960000 \t dev loss: 8.057140103464118 \t train loss: 0.009552775132677474\n",
      "step 1970000 \t dev loss: 8.053532854757172 \t train loss: 0.009540183927925553\n",
      "step 1980000 \t dev loss: 8.035273874414253 \t train loss: 0.009620802275106208\n",
      "step 1990000 \t dev loss: 8.049570800630157 \t train loss: 0.009521939812631242\n",
      "step 2000000 \t dev loss: 8.05535142071831 \t train loss: 0.009498284839055121\n",
      "evaluating iterative_solution...\n",
      "gradient_descent_soln \t train loss: 0.00949785185106947, dev_loss: 8.055350987730325 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "class Scaler():\n",
    "#     mean \n",
    "#     std \n",
    "    # hint: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "    def __call__(self,features, is_train=False):\n",
    "#         if(is_train):\n",
    "#             Scaler.mean = np.mean(features,axis=0)\n",
    "#             Scaler.std = np.std(features,axis=0)\n",
    "#         print(features.shape)\n",
    "#         features[:,0:] = (features[:,0:] - Scaler.mean)/ Scaler.std\n",
    "        '''Normalization(MinMaxScalar)'''\n",
    "        min_f = np.min(features[:,0:])\n",
    "        max_f = np.max(features[:,0:])\n",
    "        features[:,0:] = (features[:,0:]-min_f)/(max_f-min_f)\n",
    "        return features\n",
    "\n",
    "\n",
    "def get_features(csv_path,is_train=False,scaler=None):\n",
    "    '''\n",
    "    Description:\n",
    "    read input feature columns from csv file\n",
    "    manipulate feature columns, create basis functions, do feature scaling etc.\n",
    "    return a feature matrix (numpy array) of shape m x n \n",
    "    m is number of examples, n is number of features\n",
    "    return value: numpy array\n",
    "    '''\n",
    "    '''\n",
    "    Arguments:\n",
    "    csv_path: path to csv file\n",
    "    is_train: True if using training data (optional)\n",
    "    scaler: a class object for doing feature scaling (optional)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    help:\n",
    "    useful links: \n",
    "        * https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "        * https://www.geeksforgeeks.org/python-read-csv-using-pandas-read_csv/\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path)\n",
    "    features = np.array(df.drop(' shares',axis = 1))\n",
    "    '''Scaling'''\n",
    "    scaler.__call__(features,is_train)\n",
    "    '''Bias column addtition'''\n",
    "    bias = np.ones((1,len(features[:,0])))\n",
    "    features = np.insert(features, 0, bias, axis=1)\n",
    "    return features\n",
    "    \n",
    "    \n",
    "\n",
    "def get_targets(csv_path):\n",
    "    '''\n",
    "    Description:\n",
    "    read target outputs from the csv file\n",
    "    return a numpy array of shape m x 1\n",
    "    m is number of examples\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path)\n",
    "    target = df[' shares'].values.reshape(-1,1)\n",
    "    Tnorm = target.copy()\n",
    "    max_t = np.max(target)\n",
    "    min_t = np.min(target)\n",
    "    Tnorm = (target-min_t)/(max_t - min_t)\n",
    "    return Tnorm\n",
    "     \n",
    "\n",
    "def analytical_solution(feature_matrix, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    implement analytical solution to obtain weights\n",
    "    as described in lecture 5d\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "    p1 = np.dot(feature_matrix.T,feature_matrix)\n",
    "    p2 = p1 + C*np.identity(len(feature_matrix[0]))\n",
    "    p3 = np.dot(feature_matrix.T,targets)\n",
    "    w_star = np.matmul(np.linalg.pinv(p2),p3)\n",
    "    return w_star\n",
    "\n",
    "def get_predictions(feature_matrix, weights):\n",
    "    '''\n",
    "    description\n",
    "    return predictions given feature matrix and weights\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "\n",
    "    pre = feature_matrix.dot(weights)\n",
    "    return pre\n",
    "\n",
    "def mse_loss(feature_matrix, weights, targets):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement mean squared error loss function\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "    loss = np.sum((feature_matrix.dot(weights)-targets)**2,axis=0)[0]\n",
    "    return loss  \n",
    "\n",
    "def l2_regularizer(weights):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement l2 regularizer\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "\n",
    "def loss_fn(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute the loss function: mse_loss + C * l2_regularizer\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "    loss = mse_loss(feature_matrix, weights, targets) + C*(np.dot(weights.T,weights))[0,0]\n",
    "    return loss\n",
    "    \n",
    "def compute_gradients(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute gradient of weights w.r.t. the loss_fn function implemented above\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: numpy array\n",
    "    '''\n",
    "    m = len(targets)\n",
    "    grad = (2/m)*np.dot(feature_matrix.T,(np.dot(feature_matrix,weights) - targets))\n",
    "    grad = grad + 2*C*weights\n",
    "    return grad\n",
    "\n",
    "def sample_random_batch(feature_matrix, targets, batch_size):\n",
    "    '''\n",
    "    Description\n",
    "    Batching -- Randomly sample batch_size number of elements from feature_matrix and targets\n",
    "    return a tuple: (sampled_feature_matrix, sampled_targets)\n",
    "    sampled_feature_matrix: numpy array of shape batch_size x n\n",
    "    sampled_targets: numpy array of shape batch_size x 1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    batch_size: int\n",
    "    '''    \n",
    "    lb = np.random.randint(0, len(feature_matrix) - batch_size)\n",
    "    ub = lb + batch_size\n",
    "    return (feature_matrix[lb:ub], targets[lb:ub])\n",
    "    \n",
    "    \n",
    "def initialize_weights(n):\n",
    "    '''\n",
    "    Description:\n",
    "    initialize weights to some initial values\n",
    "    return value: numpy array of shape n x 1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    n: int\n",
    "    '''\n",
    "    weights = np.random.randn(n,1)\n",
    "    return weights\n",
    "    \n",
    "def update_weights(weights, gradients, lr):\n",
    "    '''\n",
    "    Description:\n",
    "    update weights using gradient descent\n",
    "    retuen value: numpy matrix of shape nx1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    # weights: numpy matrix of shape nx1\n",
    "    # gradients: numpy matrix of shape nx1\n",
    "    # lr: learning rate\n",
    "    '''    \n",
    "    weights = weights - lr*gradients\n",
    "    return weights\n",
    "\n",
    "def early_stopping(arg_1=None, arg_2=None, arg_3=None, arg_n=None):\n",
    "    # allowed to modify argument list as per your need\n",
    "    # return True or False\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def do_gradient_descent(train_feature_matrix,  \n",
    "                        train_targets, \n",
    "                        dev_feature_matrix,\n",
    "                        dev_targets,\n",
    "                        lr=1.0,\n",
    "                        C=0.0,\n",
    "                        batch_size=32,\n",
    "                        max_steps=10000,\n",
    "                        eval_steps=5):\n",
    "    '''\n",
    "    feel free to significantly modify the body of this function as per your needs.\n",
    "    ** However **, you ought to make use of compute_gradients and update_weights function defined above\n",
    "    return your best possible estimate of LR weights\n",
    "    a sample code is as follows -- \n",
    "    '''\n",
    "    weights = initialize_weights(len(train_feature_matrix[0]))\n",
    "    dev_loss = loss_fn(dev_feature_matrix, weights, dev_targets, C)\n",
    "    train_loss = loss_fn(train_feature_matrix, weights, train_targets, C)\n",
    "\n",
    "    print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(0,dev_loss,train_loss))\n",
    "    for step in range(1,max_steps+1):\n",
    "\n",
    "        #sample a batch of features and gradients\n",
    "        features,targets = sample_random_batch(train_feature_matrix,train_targets,batch_size)\n",
    "        \n",
    "        #compute gradients\n",
    "        gradients = compute_gradients(features, weights, targets, C)\n",
    "        \n",
    "        #update weights\n",
    "        weights = update_weights(weights, gradients, lr)\n",
    "\n",
    "        if step%eval_steps == 0:\n",
    "            dev_loss = loss_fn(dev_feature_matrix, weights, dev_targets, C)\n",
    "            train_loss = loss_fn(train_feature_matrix, weights, train_targets, C)\n",
    "            print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(step,dev_loss,train_loss))\n",
    "\n",
    "        '''\n",
    "        implement early stopping etc. to improve performance.\n",
    "        '''\n",
    "\n",
    "    return weights\n",
    "\n",
    "def do_evaluation(feature_matrix, targets, weights):\n",
    "    # your predictions will be evaluated based on mean squared error \n",
    "    predictions = get_predictions(feature_matrix, weights)\n",
    "    loss =  mse_loss(feature_matrix, weights, targets)\n",
    "    return loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scaler = Scaler() #use of scaler is optional\n",
    "    train_features, train_targets = get_features('train.csv',True,scaler), get_targets('train.csv')\n",
    "    dev_features, dev_targets = get_features('dev.csv',False,scaler), get_targets('dev.csv')\n",
    "\n",
    "    a_solution = analytical_solution(train_features, train_targets, C=1e-8)\n",
    "    print('evaluating analytical_solution...')\n",
    "    dev_loss=do_evaluation(dev_features, dev_targets, a_solution)\n",
    "    train_loss=do_evaluation(train_features, train_targets, a_solution)\n",
    "    print('analytical_solution \\t train loss: {:.10f}, dev_loss: {:.10f} '.format(train_loss, dev_loss))\n",
    "\n",
    "    print('training LR using gradient descent...')\n",
    "    gradient_descent_soln = do_gradient_descent(train_features, \n",
    "                        train_targets, \n",
    "                        dev_features,\n",
    "                        dev_targets,\n",
    "                        lr=0.1,\n",
    "                        C=1e-8,\n",
    "                        batch_size=32,\n",
    "                        max_steps=2000000,\n",
    "                        eval_steps=10000)\n",
    "\n",
    "    print('evaluating iterative_solution...')\n",
    "    dev_loss=do_evaluation(dev_features, dev_targets, gradient_descent_soln)\n",
    "    train_loss=do_evaluation(train_features, train_targets, gradient_descent_soln)\n",
    "    print('gradient_descent_soln \\t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "fx = df.to_numpy()\n",
    "# Normalize test data\n",
    "min_fx = np.min(fx[:,0:])\n",
    "max_fx = np.max(fx[:,0:])\n",
    "fx[:,0:] = (fx[:,0:]-min_fx)/(max_fx-min_fx)\n",
    "# Adding bias\n",
    "bias_x = np.ones((1,len(fx[:,0])))\n",
    "fx = np.insert(fx, 0, bias_x, axis=1)\n",
    "\n",
    "# Get predictions\n",
    "predictions = get_predictions(fx, gradient_descent_soln)\n",
    "\n",
    "# Denormalize using Train data min and max\n",
    "df = pd.read_csv('train.csv')\n",
    "target = df[' shares'].values.reshape(-1,1)\n",
    "max_t = np.max(target)\n",
    "min_t = np.min(target)\n",
    "predictions = (predictions*(max_t - min_t))+min_t\n",
    "\n",
    "# Writing to 'submission.csv'\n",
    "csv = pd.DataFrame()\n",
    "csv['instance_id'] = [i for i in range(len(predictions))]\n",
    "csv['shares'] = predictions\n",
    "csv.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import tkinter\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (23786, 1) \tfeatures shape: (23786, 59)\n",
      "targets shape: (23786, 1) \tfeatures shape: (23786, 60)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "features = train.values\n",
    "target = train[' shares'].values.reshape(-1,1)\n",
    "features = np.delete(features, 59, 1)\n",
    "# target = np.delete(target, np.s_[99:23786:1], 0)\n",
    "# features = np.delete(features, np.s_[99:23786:1], 0)\n",
    "print(\"targets shape:\",target.shape,\"\\tfeatures shape:\",features.shape)\n",
    "# print(\"vector=\",features[:,2])\n",
    "# print(\"mean=\",np.mean(features[:,2]),\"std=\",np.std(features[:,2]))\n",
    "\n",
    "# trial = (features[:,2] - np.mean(features[:,2])) / np.std(features[:,2])\n",
    "# mean = np.mean(features,axis=0)\n",
    "# std = np.std(features,axis=0)\n",
    "# features[:,0:] = (features[:,0:] - mean)/ std\n",
    "min_f = np.min(features[:,0:])\n",
    "max_f = np.max(features[:,0:])\n",
    "features[:,0:] = (features[:,0:]-min_f)/(max_f-min_f)\n",
    "# print(np.mean(features,axis=0))\n",
    "# print(np.std(features,axis=0))\n",
    "# max_t = np.max(features[:,2])\n",
    "# min_t = np.min(features[:,2])\n",
    "# print(\"mean=\",np.mean(features,axis=0),\"std=\",np.std(features[:,2]))\n",
    "# features[:,2] = (features[:,2]-min_t)/(max_t - min_t)\n",
    "\n",
    "# # print(\"vector=\",features[:,2])\n",
    "# print(\"mean=\",np.mean(features[:,2]),\"std=\",np.std(features[:,2]))\n",
    "# print(train.head())\n",
    "# list = []\n",
    "# for i in range(0,23786):\n",
    "#     list.append(i)\n",
    "# X = np.array(list).reshape(-1,1)\n",
    "# print(X.shape)\n",
    "# fig = plt.figure(1, figsize=(12,12))\n",
    "# ax = fig.add_subplot(121)\n",
    "# ax1 = fig.add_subplot(122)\n",
    "# ax.scatter(X,features[:,2],c='g')\n",
    "# ax1.scatter(X,features[:,2],c='r')\n",
    "# plt.show()\n",
    "bias = np.ones((1,len(features[:,0])))\n",
    "features = np.insert(features,0,bias,axis=1)\n",
    "# print(X[0:3,0:3])\n",
    "# print(np.std(X,axis=0))\n",
    "# print(target[1:4])\n",
    "# Tnorm = target.copy()\n",
    "max_t = np.max(target)\n",
    "min_t = np.min(target)\n",
    "target = (target-min_t)/(max_t - min_t)\n",
    "# max_t1 = np.max(Tnorm)\n",
    "# min_t1 = np.min(Tnorm)\n",
    "# print(min_t,max_t)\n",
    "# print(min_t1,max_t1)\n",
    "# weights = np.random.randn(len(features[0]),1)\n",
    "# print (0*weights)\n",
    "print(\"targets shape:\",target.shape,\"\\tfeatures shape:\",features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 1) \tX.W - Y == (23786, 1) \t(X.W-Y)*X.T == (60, 1)\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.randn(len(features[0,:]),1)\n",
    "m = len(target)\n",
    "p1 = features.dot(weights) - target\n",
    "p2 = np.dot(features.T,p1)\n",
    "# p3 = np.sum(p2,axis=0)\n",
    "# p1 = (feature_matrix.dot(weights) - targets)*feature_matrix\n",
    "# p2 = p1 + C*weights\n",
    "# grad = 1/m*np.sum((,axis=0) + C*weights).reshape(-1,1)\n",
    "# grad = 1/m*np.sum(p2,axis=0).reshape(-1,1)\n",
    "\n",
    "\n",
    "    \n",
    "# loss = np.subtract(np.dot(feature_matrix,weights),targets)\n",
    "# gradient_w = np.dot(feature_matrix.T,loss) \n",
    "# return gradient_w\n",
    "print(weights.shape,\"\\tX.W - Y ==\",p1.shape,\"\\t(X.W-Y)*X.T ==\",p2.shape)\n",
    "# print(\"sum() ==\",p3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xnorm= (23786, 60) ynorm= (23786, 1)\n",
      "theta= (60, 1)\n",
      "ypred= (23786, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((np.ones((len(features),1)),features),axis=1)\n",
    "Xnorm = X.copy()\n",
    "minx = np.min(X[:,1:])\n",
    "maxx = np.max(X[:,1:])\n",
    "Xnorm[:,1:] = (X[:,1:]-minx)/(maxx-minx)\n",
    "\n",
    "ynorm = target.copy()\n",
    "maxy = np.max(target)\n",
    "miny = np.min(target)\n",
    "ynorm = (target-miny)/(maxy - miny) \n",
    "print(\"xnorm=\",Xnorm.shape,\"ynorm=\",ynorm.shape)\n",
    "\n",
    "theta0 = np.zeros((X.shape[1],1))+0.4\n",
    "print(\"theta=\",theta0.shape)\n",
    "\n",
    "ypred = Xnorm.dot(theta0)\n",
    "print(\"ypred=\",ypred.shape)\n",
    "sortidx = np.argsort(ynorm[:,0]) # sort the values for better visualization\n",
    "plt.plot(ynorm[sortidx,0],'o')\n",
    "plt.plot(ypred[sortidx,0],'--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient\n",
    "def grad(theta):\n",
    "    dJ = 1/m*np.sum((Xnorm.dot(theta)-ynorm)*Xnorm,axis=0).reshape(-1,1)\n",
    "    return dJ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10360.122353200131"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost(theta):\n",
    "    J = np.sum((Xnorm.dot(theta)-ynorm)**2,axis=0)[0]\n",
    "    return J\n",
    "\n",
    "cost(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(theta0,learning_rate = 0.5,epochs=50000,TOL=1e-7):\n",
    "    \n",
    "    theta_history = [theta0]\n",
    "    cost_history = [cost(theta0)]\n",
    "    \n",
    "    theta_star = theta0*10000\n",
    "    print(f'epoch \\t Cost(J) \\t')\n",
    "    for epoch in range(epochs):\n",
    "        if epoch%100 == 0:\n",
    "            print(f'{epoch:5d}\\t{cost_history[-1]:7.4f}\\t')\n",
    "        dJ = grad(theta0)\n",
    "        J = cost(theta0)\n",
    "        \n",
    "        theta_star = theta0 - learning_rate*dJ\n",
    "        theta_history.append(theta_star)\n",
    "        cost_history.append(J)\n",
    "        \n",
    "        if np.sum((theta_star - theta0)**2) < TOL:\n",
    "            print('Convergence achieved.')\n",
    "            break\n",
    "        theta0 = theta_star\n",
    "\n",
    "    return theta_star,theta_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch \t Cost(J) \t\n",
      "    0\t10360.1224\t\n",
      "  100\t13.9842\t\n",
      "  200\t 8.5113\t\n",
      "  300\t 5.7978\t\n",
      "  400\t 4.1081\t\n",
      "  500\t 2.9595\t\n",
      "Convergence achieved.\n"
     ]
    }
   ],
   "source": [
    "theta,theta_history,cost_history = GD(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprednorm = Xnorm.dot(theta)\n",
    "\n",
    "ypred = yprednorm*(maxy-miny) + miny\n",
    "plt.plot(target[sortidx,0],'o')\n",
    "plt.plot(ypred[sortidx,0],'--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "# x = np.array(x)\n",
    "# y = np.array(y).reshape((100,1))\n",
    "# loss = np.subtract(np.dot(x,np.random.randn(len(x[0]),1)),y)\n",
    "df = pd.read_csv('train.csv')\n",
    "targets = df[' shares'].values.reshape(-1,1)\n",
    "\n",
    "feature_matrix = Xnorm.copy()\n",
    "\n",
    "# gradient_w = -2*np.dot(x.T,loss)\n",
    "# gradient_w * 2 * 0.1\n",
    "p1 = np.dot(feature_matrix.T,feature_matrix)\n",
    "p2 = p1 + 0.4*np.identity(len(feature_matrix[0]))\n",
    "p3 = np.dot(feature_matrix.T,targets)\n",
    "w_star = np.matmul(np.linalg.pinv(p2),p3)\n",
    "\n",
    "ypred=np.dot(feature_matrix,w_star)\n",
    "\n",
    "plt.plot(targets[sortidx,0],'o')\n",
    "plt.plot(ypred[sortidx,0],color='red')\n",
    "plt.title(\"Anal Solution\")\n",
    "plt.xlabel(\"Instances\")\n",
    "plt.ylabel(\"Targets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
